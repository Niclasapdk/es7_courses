# %%
import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.neural_network import MLPClassifier as MLPC
import time
import warnings
warnings.simplefilter(action='ignore') #Ignore warnings

# %% [markdown]
# # Exercise lecture 8 - Multilayer Perceptron
# Develop an MLP for the MNIST
# database by using the dimension-reduced data from your work on Exercises 2 and 3. You can download the LDA projected data here.
# Further, you can use 10-, 20- and 30-dimensional data generated by PCA and compare
# their performance (at the same time, try various MLP architectures).
# %% [markdown]
# ## Loading data
# %%
def create_complete_datasets(data_dict):
    '''
    Function for creating complete training and test sets containing
    all classes.
    '''
    #Empty list
    trainset = []
    traintargets =[]
    testset = []
    testtargets =[]
   
    #For each class
    for i in range(10):
        trainset.append(data_dict["train%d"%i])
        traintargets.append(np.full(len(data_dict["train%d"%i]),i))
        testset.append(data_dict["test%d"%i])
        testtargets.append(np.full(len(data_dict["test%d"%i]),i))
   
    #Concatenate into to complete datasets
    trainset = np.concatenate(trainset)
    traintargets = np.concatenate(traintargets)
    testset = np.concatenate(testset)
    testtargets = np.concatenate(testtargets)
    return trainset, traintargets, testset, testtargets

file = "mnist_all.mat"
data = loadmat(file)
#Complete training and test sets
train_set, train_targets, test_set, test_targets = create_complete_datasets(data)
train_set = train_set/255
test_set = test_set/255
classes = np.arange(10)

# %% [markdown]
# ## Generating PCA/LDA data
# From previous exercises we already know how to use PCA/LDA.
# We first fit a PCA/LDA model to our training data, and then transform the training and test data using this model, to get a dimensionality reduced data set.
# %%

# Load LDA data
lda_data = loadmat("mnist_lda.mat")
train_lda = lda_data['train_data']
test_lda = lda_data['test_data']

print(f"LDA Training data shape: {train_lda.shape}")
print(f"LDA Test data shape: {test_lda.shape}")

# Generate PCA data with different dimensions
pca_dimensions = [10, 20, 30]
pca_data = {}

for n_components in pca_dimensions:
    print(f"\nGenerating PCA with {n_components} components...")
    pca = PCA(n_components=n_components)
    train_pca = pca.fit_transform(train_set)
    test_pca = pca.transform(test_set)
    
    pca_data[n_components] = {
        'train': train_pca,
        'test': test_pca,
        'explained_variance': np.sum(pca.explained_variance_ratio_)
    }
    
    print(f"PCA-{n_components} Training data shape: {train_pca.shape}")
    print(f"PCA-{n_components} Test data shape: {test_pca.shape}")
    print(f"PCA-{n_components} Explained variance: {pca_data[n_components]['explained_variance']:.4f}")

# %% [markdown]
# ## Creating and Training MLP
# Sklearn has a multilayer perceptron classifier which we can use:
# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
#
# To use it, we need to choose how many layers we would like to use and the size of each hidden layer.
# We can also choose which non-linear activation function to use and what optimizer/solver to use.
# You can set a maximum iteration number as well (to limit compute time).
# %% [markdown]
# ### Model trained on LDA data
# %%
# Instantiate MLP with your parameters of choice and train it on the LDA dimensionality reduced training data.
# Afterwards test it on the test data

print("\n" + "="*60)
print("Training MLP on LDA data")
print("="*60)

# Create MLP for LDA
mlp_lda = MLPC(
    hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 neurons
    activation='relu',              # ReLU activation function
    solver='adam',                  # Adam optimizer
    max_iter=200,                   # Maximum iterations
    random_state=42,
    verbose=False
)

# Train on LDA data
start_time = time.time()
mlp_lda.fit(train_lda, train_targets)
train_time_lda = time.time() - start_time

# Test on LDA data
train_accuracy_lda = mlp_lda.score(train_lda, train_targets)
test_accuracy_lda = mlp_lda.score(test_lda, test_targets)
predictions_lda = mlp_lda.predict(test_lda)

print(f"Training time: {train_time_lda:.2f} seconds")
print(f"Training accuracy: {train_accuracy_lda:.4f}")
print(f"Test accuracy: {test_accuracy_lda:.4f}")

# %% [markdown]
# ### Model trained on PCA data
# %%
# Instantiate MLP with your parameters of choice and train it on the PCA dimensionality reduced training data.
# Afterwards test it on the test data

mlp_pca = {}
predictions_pca = {}
results_pca = {}

for n_components in pca_dimensions:
    print("\n" + "="*60)
    print(f"Training MLP on PCA data ({n_components} components)")
    print("="*60)
    
    # Create MLP for PCA
    mlp = MLPC(
        hidden_layer_sizes=(100, 50),  # Two hidden layers
        activation='relu',
        solver='adam',
        max_iter=200,
        random_state=42,
        verbose=False
    )
    
    # Train on PCA data
    start_time = time.time()
    mlp.fit(pca_data[n_components]['train'], train_targets)
    train_time = time.time() - start_time
    
    # Test on PCA data
    train_accuracy = mlp.score(pca_data[n_components]['train'], train_targets)
    test_accuracy = mlp.score(pca_data[n_components]['test'], test_targets)
    predictions = mlp.predict(pca_data[n_components]['test'])
    
    # Store results
    mlp_pca[n_components] = mlp
    predictions_pca[n_components] = predictions
    results_pca[n_components] = {
        'train_time': train_time,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy
    }
    
    print(f"Training time: {train_time:.2f} seconds")
    print(f"Training accuracy: {train_accuracy:.4f}")
    print(f"Test accuracy: {test_accuracy:.4f}")

# %% [markdown]
# ## Comparing the trained models
# With models trained on both LDA and PCA data, let's compare them using the confusion matrices.
# %%
#%%Confusion matrix
#Compute the confusion matrices for both MLPs (trained on PCA and LDA) and plot

# Create figure for confusion matrices (normalized to percentages)
fig, axes = plt.subplots(2, 2, figsize=(16, 14))
fig.suptitle('Confusion Matrices for MLP Models (Normalized - Row Percentages)', fontsize=16, fontweight='bold')

# LDA confusion matrix - normalized
cm_lda = confusion_matrix(test_targets, predictions_lda)
cm_lda_normalized = cm_lda.astype('float') / cm_lda.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages
disp_lda = ConfusionMatrixDisplay(confusion_matrix=cm_lda_normalized, display_labels=classes)
disp_lda.plot(ax=axes[0, 0], cmap='Blues', colorbar=True, values_format='.1f')
axes[0, 0].set_title(f'LDA (Test Accuracy: {test_accuracy_lda:.4f})', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Predicted Label')
axes[0, 0].set_ylabel('True Label')

# PCA confusion matrices - normalized
pca_axes = [axes[0, 1], axes[1, 0], axes[1, 1]]
for idx, n_components in enumerate(pca_dimensions):
    cm_pca = confusion_matrix(test_targets, predictions_pca[n_components])
    cm_pca_normalized = cm_pca.astype('float') / cm_pca.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages
    disp_pca = ConfusionMatrixDisplay(confusion_matrix=cm_pca_normalized, display_labels=classes)
    disp_pca.plot(ax=pca_axes[idx], cmap='Greens', colorbar=True, values_format='.1f')
    test_acc = results_pca[n_components]['test_accuracy']
    pca_axes[idx].set_title(f'PCA-{n_components} (Test Accuracy: {test_acc:.4f})', 
                            fontsize=12, fontweight='bold')
    pca_axes[idx].set_xlabel('Predicted Label')
    pca_axes[idx].set_ylabel('True Label')

plt.tight_layout()
plt.show()

# %% [markdown]
# ### Summary comparison
# %%

print("\n" + "="*80)
print("SUMMARY: Comparison of All Models")
print("="*80)

print(f"\n{'Method':<15} {'Dimensions':<12} {'Train Acc':<12} {'Test Acc':<12} {'Train Time (s)':<15}")
print("-" * 80)

print(f"{'LDA':<15} {train_lda.shape[1]:<12} {train_accuracy_lda:<12.4f} "
      f"{test_accuracy_lda:<12.4f} {train_time_lda:<15.2f}")

for n_components in pca_dimensions:
    r = results_pca[n_components]
    print(f"{'PCA-' + str(n_components):<15} {n_components:<12} {r['train_accuracy']:<12.4f} "
          f"{r['test_accuracy']:<12.4f} {r['train_time']:<15.2f}")

print("="*80)

# %% [markdown]
# ### What can we conclude from the comparison?
#
